{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suanda datamizin icinde zoomlu resimler yok o yuzden image generetor yardimi ile zoom in & out yaparak zenginlestiricez.\n",
    "\n",
    "path = \"myData\"\n",
    "\n",
    "myList = os.listdir(path)\n",
    "noOfClasses = len(myList)\n",
    "\n",
    "print(\"Label(sınıf) sayısı: \",noOfClasses)\n",
    "\n",
    "\n",
    "images = []\n",
    "classNo = []\n",
    "\n",
    "#burada klasorun icindeki her dosyaya ve dosyalarin icindeki her resme ulasiyoruz.\n",
    "for i in range(noOfClasses):\n",
    "    myImageList = os.listdir(path + \"\\\\\"+str(i))\n",
    "    for j in myImageList:\n",
    "        img = cv2.imread(path + \"\\\\\" + str(i) + \"\\\\\" + j)\n",
    "        #resimleri resize ederek 32 ye 32 yapiyoruz cunku bizim cnn egitimimizin girdileri 32 ye 32 olucak.\n",
    "        img = cv2.resize(img, (32,32))\n",
    "        images.append(img)\n",
    "        classNo.append(i)\n",
    "        \n",
    "print(len(images))\n",
    "print(len(classNo))\n",
    "\n",
    "#bundan sonra imagesleri np arraye ceviriyortuz cunku bundna sonra np arrayle islemler yapilabiliyor.\n",
    "images = np.array(images)\n",
    "classNo = np.array(classNo)\n",
    "\n",
    "print(images.shape)\n",
    "print(classNo.shape)\n",
    "\n",
    "\n",
    "#veriyi ayırma islemi, ilk once train ve test olarak 2 ye ayiriyoruz sonrasinda ise tekrardan\n",
    "#train veri setimizi 2 ye ayiriyoruz ve train ve validation elde edicez.\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, classNo, test_size = 0.5, random_state = 42)\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#bunun mantigi ise sudur, verimizi train ve test olarak 2 ye boluyoruz, sonrasinda bu testi sakliyoruz. egitimin sonuna kadar\n",
    "#gormeyecek. veriyi egitirken validation yapmamiz gerekecek yani dogrulama yapmamiz gerekecek. veriyi x tarin ve \n",
    "#x validaytion kullanarak egiticez en sonda verimiz hazir dedigimiz anda ise test verisetini kullanarak \n",
    "#dogrulama islemlerimizi gerceklestirecegiz.\n",
    "\n",
    "print(images.shape)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_validation.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#burada elimizdeki verileri gorsellestiriyoruz.\n",
    "\n",
    "# fig, axes = plt.subplots(3,1,figsize=(7,7))\n",
    "# fig.subplots_adjust(hspace = 0.5)\n",
    "# sns.countplot(y_train, ax = axes[0])\n",
    "# axes[0].set_title(\"y_train\")\n",
    "\n",
    "# sns.countplot(y_test, ax = axes[1])\n",
    "# axes[1].set_title(\"y_test\")\n",
    "\n",
    "# sns.countplot(y_validation, ax = axes[2])\n",
    "# axes[2].set_title(\"y_validation\")\n",
    "\n",
    "\n",
    "\n",
    "#simdi ise preprocess islemlerimizi yapiyoruz\n",
    "def preProcess(img):\n",
    "    #resmimizi gray scale a ceviriyoruz.\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #histogramimizi 0 255 arasinda genislettim ve sonrasinda\n",
    "    img = cv2.equalizeHist(img)\n",
    "    #0 255 arasi genislettigimiz histogramimizi tekrardan normalize ederek 0 ve 1 arasina getirdik.\n",
    "    img = img /255\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "#gray scale e cevirdigimiz resimlere bakiyoruz, veri gorsellestirmesini yapiyoruz. boyutunu da buyuttuk gormek icin tabi.\n",
    "# idx = 311\n",
    "# img = preProcess(x_train[idx])\n",
    "# img = cv2.resize(img,(300,300))\n",
    "# cv2.imshow(\"Preprocess \",img)\n",
    "\n",
    "\n",
    "\n",
    "#simdi bu preProcces isllemini tum verimize uygulayalim.\n",
    "#map fonksiyonu suna yariyor; 2 tane parametre aliyor ilki bir fonksiyon ikincisi bir lliste. ilk parametredeki fonksiyonu\n",
    "#2. parametredeki llisteye uygulamamiza yariyor.\n",
    "x_train = np.array(list(map(preProcess, x_train)))\n",
    "x_test = np.array(list(map(preProcess, x_test)))\n",
    "x_validation = np.array(list(map(preProcess, x_validation)))\n",
    "\n",
    "#burdaki -1 x trainin boyutuna gore kendisini ayarlamasini gerektigi anllamina geliyor, geri kalanlari 32 32 1 yaptiriyoruz.\n",
    "#yani ordaki -1 train listemizde kac eleman oldugunun anlamina geiyor.\n",
    "x_train = x_train.reshape(-1,32,32,1)\n",
    "print(x_train.shape)\n",
    "x_test = x_test.reshape(-1,32,32,1)\n",
    "x_validation = x_validation.reshape(-1,32,32,1)\n",
    "\n",
    "\n",
    "\n",
    "#data generate isllemi\n",
    "#width_shift_range = 0.1, sudur ki; 0.1 oraninda shift ediyor yani kaydir anlamina geliyor.\n",
    "#height de yukseklikte kaydrma anlamina geliyor.\n",
    "#zoom range ise adi ustunde yakinlalstirma skallasi, bunun daha da artmasinda fayda var. bu suan yetersiz gelecek.\n",
    "#rotation range ise dondurme isllemi onu da 10 yapiyoruz.\n",
    "dataGen = ImageDataGenerator(width_shift_range = 0.1,\n",
    "                             height_shift_range = 0.1,\n",
    "                             zoom_range = 0.1,\n",
    "                             rotation_range = 10)\n",
    "\n",
    "dataGen.fit(x_train)\n",
    "\n",
    "#burada da train ve vallidation datallarimizi one hot encoding hale getiriyoruz.\n",
    "y_train = to_categorical(y_train, noOfClasses)\n",
    "y_test = to_categorical(y_test, noOfClasses)\n",
    "y_validation = to_categorical(y_validation, noOfClasses)\n",
    "\n",
    "#modeimizi insa edelim, illk once sequentiall ille katmanlari olusturuyoruz.\n",
    "model = Sequential()\n",
    "#sonrasinda conv katmaninda input shape veriyoruz, filtreyi 8 yapiyoruz ksize imizi da 5 e 5 yapiyoruz padding piksell ekleme.\n",
    "model.add(Conv2D(input_shape = (32,32,1), filters = 8, kernel_size = (5,5), activation = \"relu\", padding = \"same\"))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "#bir sonraki katmanda ise filtreyi 16 ksize imizi 3 e 3 yapiyouz.\n",
    "model.add(Conv2D( filters = 16, kernel_size = (3,3), activation = \"relu\", padding = \"same\"))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "#yenio veri uretigimiz icin owerfittingi engellemek icin dropout ekliyoruz.\n",
    "model.add(Dropout(0.2))\n",
    "#flatten ile duzestirme yapiyoruz.\n",
    "model.add(Flatten())\n",
    "#dense ile hidden layeriomizi ekliyoruz., 256 hucre verdik.\n",
    "model.add(Dense(units=256, activation = \"relu\" ))\n",
    "#1 drop out daha attik.\n",
    "model.add(Dropout(0.2))\n",
    "#cikti degerleri.\n",
    "model.add(Dense(units=noOfClasses, activation = \"softmax\" ))\n",
    "\n",
    "#onceki derste anlattik.\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer=(\"Adam\"), metrics = [\"accuracy\"])\n",
    "\n",
    "batch_size = 250\n",
    "\n",
    "#simdi egitim yapiyoruz. hist ile gorsellestirme yapicaz o yuzden histe atadik.\n",
    "hist = model.fit_generator(dataGen.flow(x_train, y_train, batch_size = batch_size), \n",
    "                                        validation_data = (x_validation, y_validation),\n",
    "                                        epochs = 15,steps_per_epoch = x_train.shape[0]//batch_size, shuffle = 1)\n",
    "\n",
    "#modelimizi depolamasi icin picklle ekledik.\n",
    "pickle_out = open(\"model_trained_new.p\",\"wb\")\n",
    "pickle.dump(model, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be2b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simdi degerlendirme asamasini yapiyoruz gorselllestirme asamasi.\n",
    "\n",
    "hist.history.keys()\n",
    "\n",
    "#burada egitim ve validation degerlerinin basari ve basarisizlik durumlarina bakiyoruz.\n",
    "plt.figure()\n",
    "plt.plot(hist.history[\"loss\"], label = \"Eğitim Loss\")\n",
    "plt.plot(hist.history[\"val_loss\"], label = \"Val Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist.history[\"accuracy\"], label = \"Eğitim accuracy\")\n",
    "plt.plot(hist.history[\"val_accuracy\"], label = \"Val accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose = 1)\n",
    "print(\"Test loss: \", score[0])\n",
    "print(\"Test accuracy: \", score[1])\n",
    "\n",
    "\n",
    "#burada x validationu kullanarak bir tahmin yaptirtiyoruz ve bunun sonucun da da karsimiza cikarn olasillliksal degerlleri\n",
    "#maksimize eden seyi bulup indeksini cekiyoruz ve bu bizim tahminimizdir diyoruz y_pred_class a atayarak.\n",
    "#bir de gercek degerine bakiyoruz,. bu da y_pred in aslinda ne oldugu baklallim y_true ille ayni mi?\n",
    "y_pred = model.predict(x_validation)\n",
    "y_pred_class = np.argmax(y_pred, axis = 1)\n",
    "Y_true = np.argmax(y_validation, axis = 1)\n",
    "\n",
    "#burda da true  ile tahminimizi karsilastiriyoruz.\n",
    "cm = confusion_matrix(Y_true, y_pred_class)\n",
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(cm, annot = True, linewidths = 0.01, cmap = \"Greens\", linecolor = \"gray\", fmt = \".1f\", ax=ax)\n",
    "plt.xlabel(\"predicted\")\n",
    "plt.ylabel(\"true\")\n",
    "plt.title(\"cm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e0a029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fbc050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f29ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e913481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e238f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7395e6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b0bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05ffb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b086a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269fd719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e914caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d16af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7adc83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746604e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10fc076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00292fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148a1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59992f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f6048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
